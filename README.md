# MediBot â€” AI Medical Chatbot (RAG)

A smart medical chatbot built fully on open-source tooling with a simple Streamlit UI. It uses HuggingFace for embeddings, FAISS (CPU) for vector storage, and either Mistral (HuggingFace Inference) or a fast, free Groq-hosted Llama model for the conversational LLM. The retrieval pipeline follows a classic RAG pattern: load PDFs â†’ chunk â†’ embed â†’ store â†’ retrieve â†’ answer.

> â—ï¸**Disclaimer:** This project is for educational use only and does **not** provide medical advice or diagnosis. Always consult qualified healthcare professionals.

---

## âœ¨ Features

- **RAG pipeline** over your own PDFs (medical notes, guidelines, etc.)
- **FAISS** local vector store on CPU for fast retrieval
- **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2` (HuggingFace)
- **LLM options:**
  - **Mistral-7B-Instruct** via HuggingFace Inference (requires HF token)
  - **meta-llama/llama-4-maverick-17b-128e-instruct** via Groq (free, fast)
- **Streamlit chat UI** with session-based history and cached vector store
- **Grounded answers** with a strict custom prompt that avoids fabrication

---

## ğŸ—ºï¸ Architecture

**Phase 1 â€” Setup Memory for LLM (Vector DB)**  
1) Load raw PDF(s) â†’ 2) Split into chunks â†’ 3) Create embeddings â†’ 4) Store in FAISS.

**Phase 2 â€” Connect Memory with LLM**  
Load LLM (Mistral or Llama), connect retriever over FAISS, build `RetrievalQA` chain.

**Phase 3 â€” Chatbot UI (Streamlit)**  
Cache FAISS store, run query through `RetrievalQA`, display answer + sources.

---

## ğŸ“ Project Layout
```
.
â”œâ”€ data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf
â”œâ”€ vectorstore/
â”‚ â””â”€ db_faiss/
|      â””â”€ index.faiss
|      â””â”€ index.pkl
|      
â”œâ”€ create_memory_for_llm.py # Phase 1: build FAISS from PDFs
â”œâ”€ connect_memory_with_llm.py # Phase 2: test RAG via console
â”œâ”€ medibot.py # Phase 3: Streamlit chat UI
â””â”€ README.md
```
---

## ğŸ§° Tech Stack

- **LangChain** (RAG orchestration)  
- **HuggingFace** (embeddings + optional Mistral endpoint)  
- **FAISS (CPU)** (vector database)  
- **Streamlit** (chat UI)  
- **Python** (core language)

---

## ğŸš€ Quickstart

### 1) Create & Activate Virtual Env (Pipenv)

```bash
pipenv install langchain langchain_community langchain_huggingface faiss-cpu pypdf
pipenv install huggingface_hub
pipenv install streamlit
pipenv shell
```

### 2) Add your PDFs

Place one or more `.pdf` files in `data/`. The loader scans `data/*.pdf`.

### 3) Configure API Keys

Create a `.env` or export environment variables before running:

```bash
# For HuggingFace Inference (Mistral)
export HF_TOKEN=your_hf_token_here

# For Groq (used by Streamlit app with Llama-4 Maverick)
export GROQ_API_KEY=your_groq_api_key_here
```

### 4) Build the Vector Store (Phase 1)

```bash
python create_memory_for_llm.py
```

What it does:
- Loads PDFs from `data/` with `DirectoryLoader` + `PyPDFLoader`  
- Splits into chunks of **500** tokens with **50** overlap  
- Embeds using **all-MiniLM-L6-v2**  
- Saves FAISS index at `vectorstore/db_faiss`

### 5a) Console Test (Mistral via HF Inference)

```bash
python connect_memory_with_llm.py
```

### 5b) Streamlit Chat UI (Groq Llama-4 Maverick)

```bash
streamlit run medibot.py
```

---

## ğŸ§ª Prompting & Guardrails

The system prompt instructs the model to **only** answer from provided context and to say â€œI donâ€™t knowâ€ if the answer isnâ€™t foundâ€”this helps avoid unsafe or made-up medical claims.

---

## âš™ï¸ Configuration Notes

- **Retriever top-k:** 3 neighbors for focused context  
- **Chunking:** 500/50 (size/overlap) tuned for small medical PDFs  
- **Model choices:**  
  - HF Inference: `mistralai/Mistral-7B-Instruct-v0.3`  
  - Groq: `meta-llama/llama-4-maverick-17b-128e-instruct`

---

## ğŸ” Security & Privacy

- **Local vectors:** FAISS index stays on disk (`vectorstore/db_faiss`).  
- **API keys:** Use environment variables; do not commit `.env` to source control.

---

## ğŸ› ï¸ Troubleshooting

- **â€œFailed to load the vector storeâ€** â†’ Run `create_memory_for_llm.py` first  
- **Mistral console errors** â†’ Ensure `HF_TOKEN` is set  
- **Groq chat errors** â†’ Ensure `GROQ_API_KEY` is set

---

